import torch
"""
y.sum().backward() 适用于计算总和标量的梯度，通常在需要优化总和损失的情况下使用。
    这行代码计算了张量 y 中所有元素的和，并将结果作为一个标量。通常在训练神经网络时，这样的标量可以表示一个损失函数或者一种代价函数。
    backward() 方法会计算标量对于计算图中所有相关张量的梯度，然后将这些梯度存储在相应张量的 .grad 属性中。这样你就可以使用这些梯度来进行梯度下降或其他优化算法，更新模型参数
y.mean().backward() 适用于计算平均值标量的梯度，通常在需要优化平均损失的情况下使用。
    这行代码计算了张量 y 中所有元素的平均值，并将结果作为一个标量。在许多机器学习任务中，使用平均损失来衡量模型的性能更为常见。
    同样地，backward() 方法会计算平均值对于计算图中所有相关张量的梯度，并将这些梯度存储在相应张量的 .grad 属性中。
"""

# 复合函数求梯度
''' 
函数公式
z = 3 *(x+2) **2

x (求梯度必须是某一点的梯度 若x=1)
y = (x+2)
z = y * y  *3
out = z.mean() = 1/4*z

方法1：计算过程
out/x | (x = 1) = (out/z ) * ( z/y ) * (y/x) | x= 1  # 注意:此处梯度连乘
                 = 1/4 * (2y) *3 * 1
                 = 1/4 * (2 (x+2)) *3 *1
                 = 1/4 * (18)* 1 = 4.5

方法2：计算过程
x = [[x1, x2],
     [x3, x4]]

y = x + 2 = [[x1 + 2, x2 + 2],
          [x3 + 2, x4 + 2]]

z = 3*y**2 = [[3*(x1 + 2)**2, 3*(x2 + 2)**2], [3*(x3 + 2)**2, 3*(x4 + 2)**2]]
out = z.mean() = 3/4*(x1 + 2)**2 + 3/4*3*(x2 + 2)**2 + 3/4*(x3 + 2)**2 + 3/4*(x4 + 2)**2
x1 导数: 6/4 * (x1 + 2)
x2 导数: 6/4 * (x2 + 2)
x3 导数: 6/4 * (x3 + 2)
x4 导数: 6/4 * (x4 + 2)
由于 x1=x2=x3=x4=1
所以:
x1 导数: 18/4 = 4.5
x2 导数: 18/4 = 4.5
x3 导数: 18/4 = 4.5
x4 导数: 18/4 = 4.5
'''


# z = 3 *(x+2) **2
def test05():
    # 定义一个张量
    x = torch.ones(2, 2, requires_grad=True)
    print("x-->", x)  # x--> tensor([[1., 1.], [1., 1.]], requires_grad=True)

    # 将x+2看做一个函数表达式
    y = x + 2
    print("y-->", y)  # y--> tensor([[3., 3.], [3., 3.]], grad_fn=<AddBackward0>)

    # 此时将原始函数表达式转换成z和y的表达式
    z = y * y * 3
    print('z->', z, z.shape)  # z-> tensor([[27., 27.], [27., 27.]], grad_fn=<MulBackward0>) torch.Size([2, 2])

    # z是一个向量张量,需要转换为标量
    out = z.mean()
    print('out-->', out, out.shape)  # out--> tensor(27., grad_fn=<MeanBackward0>) torch.Size([])


    # 计算梯度
    out.backward()
    print('x.grad-->', x.grad)  # x.grad--> tensor([[4.5000, 4.5000], [4.5000, 4.5000]])


if __name__ == '__main__':
    test05()